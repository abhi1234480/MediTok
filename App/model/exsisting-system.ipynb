{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1905968,"sourceType":"datasetVersion","datasetId":1136210}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, sys, ast, math, warnings, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport wfdb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nwarnings.filterwarnings(\"ignore\")\n\n# ---------------- CONFIG ----------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\nPTBXL_PATH = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\"\n# change above to your local path if needed\n\nMAX_LEN = 500        # sequence length (time samples). Use 500 for 100Hz => 5s; adjust as needed\nBATCH = 32\nNUM_WORKERS = 2\nRANDOM_SEED = 42\n\n# Training epoch settings for each algorithm (small for demo)\nEPOCHS_CLASSIFIER = 2\nEPOCHS_CAPTION = 1\nEPOCHS_GPT = 1\n\n# Target classes (superclasses)\nCLASSES = ['CD','HYP','MI','NORM','STTC']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:14:17.221200Z","iopub.execute_input":"2025-09-23T14:14:17.221993Z","iopub.status.idle":"2025-09-23T14:14:23.974434Z","shell.execute_reply.started":"2025-09-23T14:14:17.221965Z","shell.execute_reply":"2025-09-23T14:14:23.973774Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import math\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\n# Load metadata\nmeta_csv = os.path.join(PTBXL_PATH, \"ptbxl_database.csv\")\nscp_csv = os.path.join(PTBXL_PATH, \"scp_statements.csv\")\nassert os.path.exists(meta_csv), f\"ptbxl_database.csv not found at {meta_csv}\"\nassert os.path.exists(scp_csv), f\"scp_statements.csv not found at {scp_csv}\"\n\nmeta_df = pd.read_csv(meta_csv, index_col=0)\nscp_df = pd.read_csv(scp_csv, index_col=0)\n\n# helper to map scp_codes -> diagnostic_superclass (list)\ndef scp_to_superclasses(scp_codes_str):\n    try:\n        d = ast.literal_eval(scp_codes_str)\n    except Exception:\n        return []\n    out=[]\n    for k in d.keys():\n        if k in scp_df.index and scp_df.loc[k, 'diagnostic_class'] in CLASSES:\n            out.append(scp_df.loc[k,'diagnostic_class'])\n    return list(set(out))\n\nmeta_df['diagnostic_superclass'] = meta_df['scp_codes'].apply(scp_to_superclasses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:15:28.841346Z","iopub.execute_input":"2025-09-23T14:15:28.842210Z","iopub.status.idle":"2025-09-23T14:15:30.156622Z","shell.execute_reply.started":"2025-09-23T14:15:28.842185Z","shell.execute_reply":"2025-09-23T14:15:30.156162Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# MultiLabel binarizer\nmlb = MultiLabelBinarizer(classes=CLASSES)\nY_all = mlb.fit_transform(meta_df['diagnostic_superclass'])\n\n# Safe index mapping for filename paths (some rows store relative paths like 'records100/00000/00001_lr')\ndef get_full_path(fname):\n    # fname may already be a full path or relative to PTBXL_PATH\n    p = os.path.join(PTBXL_PATH, fname)\n    base,_ = os.path.splitext(p)\n    # prefer .dat/.hea pairs; wfdb.rdsamp accepts base path without extension\n    if os.path.exists(base + \".dat\") and os.path.exists(base + \".hea\"):\n        return base\n    # try adding ptbxl prefix if needed\n    if os.path.exists(p):\n        return os.path.splitext(p)[0]\n    # fallback: return base anyway\n    return base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:15:39.728097Z","iopub.execute_input":"2025-09-23T14:15:39.728377Z","iopub.status.idle":"2025-09-23T14:15:39.749641Z","shell.execute_reply.started":"2025-09-23T14:15:39.728356Z","shell.execute_reply":"2025-09-23T14:15:39.748951Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class PTBXL_Dataset(Dataset):\n    def __init__(self, df, Y, max_len=MAX_LEN):\n        self.df = df.reset_index(drop=True)\n        self.Y = Y\n        self.max_len = max_len\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        base = get_full_path(row['filename_lr'])\n        try:\n            sig, _ = wfdb.rdsamp(base)\n        except Exception as e:\n            # fallback: try hr\n            base2 = get_full_path(row['filename_hr']) if 'filename_hr' in row else base\n            sig, _ = wfdb.rdsamp(base2)\n        sig = sig.astype(np.float32)  # shape (T, 12)\n        # ensure shape (T,12)\n        if sig.ndim==1: sig = sig[:,None]\n        if sig.shape[1] != 12 and sig.shape[0]==12: sig = sig.T\n        if sig.shape[1] != 12:\n            # pad or trim leads dimension\n            if sig.shape[1] < 12:\n                sig = np.pad(sig, ((0,0),(0, 12 - sig.shape[1])), mode='constant')\n            else:\n                sig = sig[:, :12]\n        # normalize per lead\n        sig = (sig - sig.mean(axis=0)) / (sig.std(axis=0) + 1e-8)\n        # pad/trim in time\n        if sig.shape[0] > self.max_len:\n            sig = sig[:self.max_len, :]\n        else:\n            pad_len = self.max_len - sig.shape[0]\n            sig = np.pad(sig, ((0,pad_len),(0,0)))\n        # flip to (channels, seq_len)\n        x = torch.tensor(sig.T, dtype=torch.float)  # (12, max_len)\n        y = torch.tensor(self.Y[idx].astype(np.float32), dtype=torch.float)\n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:15:50.354344Z","iopub.execute_input":"2025-09-23T14:15:50.354627Z","iopub.status.idle":"2025-09-23T14:15:50.362483Z","shell.execute_reply.started":"2025-09-23T14:15:50.354605Z","shell.execute_reply":"2025-09-23T14:15:50.361766Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"indices = np.arange(len(meta_df))\n# filter out records with empty labels? keep them (multi-label may be empty); stratify on first non-empty label or fallback\nhas_label_mask = np.array([len(l)>0 for l in meta_df['diagnostic_superclass']])\n# For stratify, use string representation; if too rare leads to ValueError, fallback to random split\ntry:\n    train_idx, test_idx = train_test_split(indices, test_size=0.1, random_state=RANDOM_SEED, stratify=[str(s) for s in meta_df['diagnostic_superclass']])\n    train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=RANDOM_SEED, stratify=[str(meta_df.iloc[i]['diagnostic_superclass']) for i in train_idx])\nexcept Exception:\n    train_idx, test_idx = train_test_split(indices, test_size=0.1, random_state=RANDOM_SEED)\n    train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=RANDOM_SEED)\n\ntrain_df = meta_df.iloc[train_idx].copy()\nval_df = meta_df.iloc[val_idx].copy()\ntest_df = meta_df.iloc[test_idx].copy()\nY = mlb.transform(meta_df['diagnostic_superclass'])\ntrain_Y = Y[train_idx]; val_Y = Y[val_idx]; test_Y = Y[test_idx]\n\nprint(\"Sizes (train/val/test):\", len(train_df), len(val_df), len(test_df))\n\ntrain_ds = PTBXL_Dataset(train_df, train_Y, max_len=MAX_LEN)\nval_ds = PTBXL_Dataset(val_df, val_Y, max_len=MAX_LEN)\ntest_ds = PTBXL_Dataset(test_df, test_Y, max_len=MAX_LEN)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS)\nval_loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)\ntest_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:16:01.705052Z","iopub.execute_input":"2025-09-23T14:16:01.705793Z","iopub.status.idle":"2025-09-23T14:16:01.782724Z","shell.execute_reply.started":"2025-09-23T14:16:01.705764Z","shell.execute_reply":"2025-09-23T14:16:01.782007Z"}},"outputs":[{"name":"stdout","text":"Sizes (train/val/test): 17687 1966 2184\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def eval_multilabel(model, loader, threshold=0.5):\n    model.eval()\n    Ys=[]; Ps=[]\n    with torch.no_grad():\n        for x,y in loader:\n            x = x.to(DEVICE)\n            # model expects (B, C, T) - many models below follow that\n            logits = model(x)\n            probs = torch.sigmoid(logits).cpu().numpy()\n            Ys.append(y.numpy())\n            Ps.append(probs)\n    Ys = np.vstack(Ys); Ps = np.vstack(Ps)\n    pred = (Ps > threshold).astype(int)\n    # metrics per-class\n    try:\n        auc = roc_auc_score(Ys, Ps, average='macro')\n    except Exception:\n        auc = float('nan')\n    f1 = f1_score(Ys, pred, average='macro', zero_division=0)\n    acc = (pred == Ys).all(axis=1).mean()  # strict multi-label exact-match accuracy\n    print(\"AUROC(macro):\", auc)\n    print(\"F1(macro):\", f1)\n    print(\"Exact-match accuracy:\", acc)\n    return Ys, Ps, pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:16:14.696487Z","iopub.execute_input":"2025-09-23T14:16:14.697030Z","iopub.status.idle":"2025-09-23T14:16:14.702920Z","shell.execute_reply.started":"2025-09-23T14:16:14.697007Z","shell.execute_reply":"2025-09-23T14:16:14.702144Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"\\n== Algorithm 1: BPE Tokenizer (demo) ==\")\ntry:\n    from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n    sample_reports = meta_df['report'].dropna().astype(str).tolist()\n    # train small BPE on reports (demo)\n    tmp_file = \"ptbxl_reports_corpus.txt\"\n    with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n        for r in sample_reports[:5000]:\n            f.write(r.replace(\"\\n\",\" \") + \"\\n\")\n    tokenizer = Tokenizer(models.BPE())\n    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n    trainer = trainers.BpeTrainer(vocab_size=8000, special_tokens=[\"<pad>\",\"<s>\",\"</s>\",\"<unk>\"])\n    tokenizer.train([tmp_file], trainer)\n    print(\"BPE vocab size:\", tokenizer.get_vocab_size())\n    print(\"Sample tokens:\", tokenizer.encode(\"Normal sinus rhythm\").tokens[:20])\nexcept Exception as e:\n    print(\"Tokenizers not available or corpus missing:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:16:21.609251Z","iopub.execute_input":"2025-09-23T14:16:21.609796Z","iopub.status.idle":"2025-09-23T14:16:21.837413Z","shell.execute_reply.started":"2025-09-23T14:16:21.609773Z","shell.execute_reply":"2025-09-23T14:16:21.836863Z"}},"outputs":[{"name":"stdout","text":"\n== Algorithm 1: BPE Tokenizer (demo) ==\n\n\n\nBPE vocab size: 2928\nSample tokens: ['N', 'orm', 'al', 'sinus', 'rhythm']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"\\n== Algorithm 2: ResNet1D Encoder + Transformer Decoder (captioning sketch) ==\")\nfrom torch.nn import TransformerDecoderLayer, TransformerDecoder\n\nclass ResNet1D_Enc(nn.Module):\n    def __init__(self, out_dim=128):\n        super().__init__()\n        self.conv1 = nn.Conv1d(12,64, kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n        # adaptively pool to fixed sequence length\n        self.pool = nn.AdaptiveAvgPool1d(128)\n        self.fc = nn.Linear(64*128, out_dim)\n    def forward(self,x):\n        # x: (B,12,T)\n        h = self.relu(self.bn1(self.conv1(x)))\n        h = self.pool(h)       # (B,64,128)\n        h = h.flatten(1)\n        return self.fc(h)      # (B, out_dim)\n\nclass ECGCaptioner(nn.Module):\n    def __init__(self, vocab_size=1000, d_model=128):\n        super().__init__()\n        self.encoder = ResNet1D_Enc(d_model)\n        dec_layer = TransformerDecoderLayer(d_model=d_model, nhead=8)\n        self.decoder = TransformerDecoder(dec_layer, num_layers=2)\n        self.fc_out = nn.Linear(d_model, vocab_size)\n    def forward(self, x, tgt):\n        # x: (B,12,T), tgt: (tgt_len,B,d_model)\n        enc = self.encoder(x).unsqueeze(0)  # (1, B, d_model)\n        out = self.decoder(tgt, enc)\n        return self.fc_out(out)  # (tgt_len, B, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:17:19.024591Z","iopub.execute_input":"2025-09-23T14:17:19.025316Z","iopub.status.idle":"2025-09-23T14:17:19.032292Z","shell.execute_reply.started":"2025-09-23T14:17:19.025293Z","shell.execute_reply":"2025-09-23T14:17:19.031714Z"}},"outputs":[{"name":"stdout","text":"\n== Algorithm 2: ResNet1D Encoder + Transformer Decoder (captioning sketch) ==\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Create a small training loop but here we only demo training with fake targets\ncaption_vocab = 500\ncaption_model = ECGCaptioner(vocab_size=caption_vocab, d_model=128).to(DEVICE)\nopt = torch.optim.Adam(caption_model.parameters(), lr=1e-4)\ncaption_model.train()\nfor epoch in range(EPOCHS_CAPTION):\n    epoch_loss = 0.0\n    for xb, yb in train_loader:\n        xb = xb.to(DEVICE)  # (B,12,T)\n        # create fake target representations (tgt_len, B, d_model)\n        tgt_len = 10\n        tgt = torch.randn(tgt_len, xb.size(0), 128, device=DEVICE)\n        logits = caption_model(xb, tgt)    # (tgt_len, B, vocab)\n        # fake labels\n        labels = torch.randint(0, caption_vocab, (tgt_len*xb.size(0),), device=DEVICE)\n        loss = F.cross_entropy(logits.view(-1, caption_vocab), labels)\n        opt.zero_grad(); loss.backward(); opt.step()\n        epoch_loss += loss.item()\n    print(f\"Captioning epoch {epoch+1}, loss {epoch_loss/len(train_loader):.4f}\")\nprint(\"Captioning demo done. (Use real tokenized targets for real training.)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:17:52.840874Z","iopub.execute_input":"2025-09-23T14:17:52.841152Z","iopub.status.idle":"2025-09-23T14:20:22.071058Z","shell.execute_reply.started":"2025-09-23T14:17:52.841132Z","shell.execute_reply":"2025-09-23T14:20:22.070248Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Captioning epoch 1, loss 6.2467\nCaptioning demo done. (Use real tokenized targets for real training.)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"\\n== Algorithm 3: ChexNet (DenseNet121) on spectrograms + optional GPT-2 sketch ==\")\n# we will convert ECG -> mel-spectrogram per lead and stack 3 selected leads as RGB channels\ntry:\n    import librosa\n    import librosa.display\nexcept Exception:\n    librosa = None\n    print(\"librosa not installed â€” install librosa to run spectrogram-based ChexNet approach (pip install librosa).\")\n\nfrom torchvision import models, transforms\n\ndef ecg_to_melspec_batch(X_numpy, sr=100, n_mels=128, target_size=(128,128)):\n    # X_numpy: (B,12,T) numpy\n    B = X_numpy.shape[0]\n    imgs = []\n    for b in range(B):\n        leads = X_numpy[b]  # (12,T)\n        channels = []\n        # create mel for first 3 leads (I, II, V1) or choose [0,1,6]\n        lead_idxs = [0,1,6]\n        for li in lead_idxs:\n            sig = leads[li]\n            if librosa is None:\n                # fallback simple STFT using numpy (not optimal)\n                S = np.abs(np.fft.rfft(sig, n=target_size[1]*2))[:target_size[1]]\n                S = np.tile(S[:target_size[1]], (target_size[0],1))\n            else:\n                S = librosa.feature.melspectrogram(y=sig, sr=sr, n_mels=target_size[0], n_fft=256, hop_length=max(1, len(sig)//target_size[1]))\n                S = librosa.power_to_db(S, ref=np.max)\n                # resize/pad to target_size\n                if S.shape[1] < target_size[1]:\n                    pad = target_size[1] - S.shape[1]\n                    S = np.pad(S, ((0,0),(0,pad)), mode='constant')\n                else:\n                    S = S[:, :target_size[1]]\n            # normalize\n            S = (S - S.mean()) / (S.std()+1e-8)\n            channels.append(S)\n        img = np.stack(channels, axis=0)  # (3, H, W)\n        imgs.append(img)\n    return np.stack(imgs, axis=0).astype(np.float32)  # (B,3,H,W)\n\n# Simple DenseNet classifier on spectrograms\nclass DenseNetSpec(nn.Module):\n    def __init__(self, num_classes=len(CLASSES), pretrained=False):\n        super().__init__()\n        base = models.densenet121(pretrained=pretrained)\n        # adapt first conv to accept 3 channels (already 3)\n        self.features = base.features\n        self.classifier = nn.Linear(base.classifier.in_features, num_classes)\n    def forward(self,x):\n        # x: (B,3,H,W)\n        f = self.features(x)\n        f = F.relu(f, inplace=True)\n        f = F.adaptive_avg_pool2d(f, (1,1)).view(x.size(0), -1)\n        return self.classifier(f)\n\nif librosa is not None:\n    # training loop for DenseNet on spectrograms (quick demo)\n    densenet = DenseNetSpec().to(DEVICE)\n    opt = torch.optim.Adam(densenet.parameters(), lr=1e-4)\n    criterion = nn.BCEWithLogitsLoss()\n    for epoch in range(EPOCHS_CLASSIFIER):\n        densenet.train(); running=0.0; n=0\n        for xb, yb in train_loader:\n            # xb: (B,12,T) -> numpy\n            xb_np = xb.numpy()\n            imgs = ecg_to_melspec_batch(xb_np, sr=100, n_mels=128, target_size=(128,128))\n            imgs_t = torch.tensor(imgs).to(DEVICE)\n            yb = yb.to(DEVICE)\n            logits = densenet(imgs_t)\n            loss = criterion(logits, yb)\n            opt.zero_grad(); loss.backward(); opt.step()\n            running += loss.item(); n+=1\n        print(f\"DenseNet spectrogram epoch {epoch+1}, loss {running/max(1,n):.4f}\")\n    # evaluate\n    def eval_densenet(loader):\n        densenet.eval()\n        Ys=[]; Ps=[]\n        with torch.no_grad():\n            for xb,yb in loader:\n                imgs = ecg_to_melspec_batch(xb.numpy())\n                imgs_t = torch.tensor(imgs).to(DEVICE)\n                probs = torch.sigmoid(densenet(imgs_t)).cpu().numpy()\n                Ys.append(yb.numpy()); Ps.append(probs)\n        Ys = np.vstack(Ys); Ps = np.vstack(Ps)\n        preds = (Ps>0.5).astype(int)\n        print(\"DenseNet AUROC:\", roc_auc_score(Ys, Ps, average='macro'))\n        print(\"DenseNet F1:\", f1_score(Ys, preds, average='macro', zero_division=0))\n    print(\"Evaluating DenseNet on test set (spectrogram)...\")\n    eval_densenet(test_loader)\nelse:\n    print(\"Skipping DenseNet spectrogram training (librosa not installed).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:20:42.756358Z","iopub.execute_input":"2025-09-23T14:20:42.757352Z","iopub.status.idle":"2025-09-23T14:29:07.172323Z","shell.execute_reply.started":"2025-09-23T14:20:42.757324Z","shell.execute_reply":"2025-09-23T14:29:07.171472Z"}},"outputs":[{"name":"stdout","text":"\n== Algorithm 3: ChexNet (DenseNet121) on spectrograms + optional GPT-2 sketch ==\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"DenseNet spectrogram epoch 1, loss 0.4346\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"DenseNet spectrogram epoch 2, loss 0.3836\nEvaluating DenseNet on test set (spectrogram)...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"DenseNet AUROC: 0.8492949527911964\nDenseNet F1: 0.5676289914979151\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"\\n== Algorithm 4: Wavelet Scattering + classifier ==\")\ntry:\n    from kymatio.torch import Scattering1D\n    has_kymatio=True\nexcept Exception:\n    has_kymatio=False\n    print(\"kymatio not installed - skip scattering. pip install kymatio to enable.\")\n\nif has_kymatio:\n    scattering = Scattering1D(J=6, shape=MAX_LEN).to(DEVICE)\n    class ScatterNet(nn.Module):\n        def __init__(self, out_dim=128, num_classes=len(CLASSES)):\n            super().__init__()\n            self.fc1 = nn.Linear(scattering.output_size(), out_dim)\n            self.classifier = nn.Linear(out_dim, num_classes)\n        def forward(self, x):\n            # x: (B,12,T) -> we'll average across leads then scatter\n            x = x.mean(dim=1)  # (B, T)\n            s = scattering(x)  # (B, C, T_sc) or (B, C)\n            if s.ndim>2:\n                s = s.mean(dim=-1)\n            h = F.relu(self.fc1(s))\n            return self.classifier(h)\n    scatter_model = ScatterNet().to(DEVICE)\n    opt = torch.optim.Adam(scatter_model.parameters(), lr=1e-4)\n    crit = nn.BCEWithLogitsLoss()\n    for epoch in range(EPOCHS_CLASSIFIER):\n        scatter_model.train(); r=0; n=0\n        for xb,yb in train_loader:\n            xb = xb.to(DEVICE)\n            yb = yb.to(DEVICE)\n            logits = scatter_model(xb)\n            loss = crit(logits, yb)\n            opt.zero_grad(); loss.backward(); opt.step()\n            r+=loss.item(); n+=1\n        print(f\"Scatter epoch {epoch+1}, loss {r/max(1,n):.4f}\")\n    print(\"Eval scattering model:\")\n    eval_multilabel(scatter_model, test_loader)\nelse:\n    print(\"Skipping scattering classifier due to missing dependency.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:29:07.174032Z","iopub.execute_input":"2025-09-23T14:29:07.174717Z","iopub.status.idle":"2025-09-23T14:29:07.184498Z","shell.execute_reply.started":"2025-09-23T14:29:07.174694Z","shell.execute_reply":"2025-09-23T14:29:07.183887Z"}},"outputs":[{"name":"stdout","text":"\n== Algorithm 4: Wavelet Scattering + classifier ==\nkymatio not installed - skip scattering. pip install kymatio to enable.\nSkipping scattering classifier due to missing dependency.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"\\n== Algorithm 5: Vision-Text Transformer (ECG-GPT sketch) ==\")\n# We build a small multimodal transformer: encoder (ResNet1D_Enc) + text decoder (simple transformer decoder),\n# then connect decoder to a small LM head. For demo, train with fake text targets.\nfrom torch.nn import Transformer\n\nclass ECGMultimodal(nn.Module):\n    def __init__(self, d_model=128, vocab_size=500):\n        super().__init__()\n        self.encoder = ResNet1D_Enc(d_model)\n        self.text_decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=8)\n        self.decoder = nn.TransformerDecoder(self.text_decoder_layer, num_layers=2)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n    def forward(self, x, tgt_emb):\n        # x: (B,12,T), tgt_emb: (tgt_len,B,d_model)\n        enc = self.encoder(x).unsqueeze(0)  # (1,B,d_model)\n        out = self.decoder(tgt_emb, enc)    # (tgt_len,B,d_model)\n        return self.lm_head(out)\n\nmmodel = ECGMultimodal().to(DEVICE)\nopt = torch.optim.Adam(mmodel.parameters(), lr=1e-4)\nfor epoch in range(EPOCHS_CAPTION):\n    mmodel.train(); r=0\n    for xb,yb in train_loader:\n        xb = xb.to(DEVICE)\n        tgt = torch.randn(12, xb.size(0), 128, device=DEVICE)\n        logits = mmodel(xb, tgt)\n        labels = torch.randint(0,500,(12*xb.size(0),), device=DEVICE)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels)\n        opt.zero_grad(); loss.backward(); opt.step()\n        r+=loss.item()\n    print(f\"Multimodal epoch {epoch+1}, loss {r/len(train_loader):.4f}\")\nprint(\"Multimodal demo done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:29:07.185319Z","iopub.execute_input":"2025-09-23T14:29:07.185596Z","iopub.status.idle":"2025-09-23T14:29:59.079703Z","shell.execute_reply.started":"2025-09-23T14:29:07.185574Z","shell.execute_reply":"2025-09-23T14:29:59.078976Z"}},"outputs":[{"name":"stdout","text":"\n== Algorithm 5: Vision-Text Transformer (ECG-GPT sketch) ==\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Multimodal epoch 1, loss 6.2436\nMultimodal demo done.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"\\n== Algorithm 6: Custom Domain-Specific Tokenizer + GPT-2 (sketch) ==\")\ntry:\n    from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n    from transformers import GPT2Config, GPT2LMHeadModel\n    # Build a tiny custom BPE on text reports (take limited subset)\n    reports = meta_df['report'].dropna().astype(str).tolist()\n    corpus_small = \"reports_small.txt\"\n    with open(corpus_small, \"w\", encoding=\"utf-8\") as f:\n        for r in reports[:2000]:\n            f.write(r.replace(\"\\n\",\" \") + \"\\n\")\n    tok = Tokenizer(models.BPE())\n    tok.pre_tokenizer = pre_tokenizers.Whitespace()\n    trainer = trainers.BpeTrainer(vocab_size=2000, special_tokens=[\"<pad>\",\"<s>\",\"</s>\",\"<unk>\"])\n    tok.train([corpus_small], trainer)\n    vocab_size = tok.get_vocab_size()\n    print(\"Custom tokenizer vocab size:\", vocab_size)\n    # Lightweight GPT2 config\n    cfg = GPT2Config(vocab_size=vocab_size, n_embd=128, n_layer=4, n_head=4)\n    gpt = GPT2LMHeadModel(cfg).to(DEVICE)\n    # quick mock training loop: use tokenized report substrings as targets (demo)\n    opt = torch.optim.Adam(gpt.parameters(), lr=1e-4)\n    for epoch in range(EPOCHS_GPT):\n        gpt.train(); r=0\n        for i,report in enumerate(reports[:500]):\n            enc = tok.encode(report)\n            ids = enc.ids\n            if len(ids)<16: continue\n            ids_t = torch.tensor(ids[:64], device=DEVICE).unsqueeze(0)\n            outputs = gpt(ids_t, labels=ids_t)\n            loss = outputs.loss\n            opt.zero_grad(); loss.backward(); opt.step()\n            r += loss.item()\n            if i>200: break\n        print(f\"GPT2-proposed epoch {epoch+1}, loss {r/200:.4f}\")\n    print(\"Custom tokenizer + GPT2 demo finished.\")\nexcept Exception as e:\n    print(\"Transformers or tokenizers not installed or corpus missing:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:29:59.081460Z","iopub.execute_input":"2025-09-23T14:29:59.081770Z","iopub.status.idle":"2025-09-23T14:30:23.416089Z","shell.execute_reply.started":"2025-09-23T14:29:59.081745Z","shell.execute_reply":"2025-09-23T14:30:23.415336Z"}},"outputs":[{"name":"stdout","text":"\n== Algorithm 6: Custom Domain-Specific Tokenizer + GPT-2 (sketch) ==\n","output_type":"stream"},{"name":"stderr","text":"2025-09-23 14:30:08.269818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758637808.600847      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758637808.701064      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n\n\nCustom tokenizer vocab size: 2000\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"GPT2-proposed epoch 1, loss 1.1076\nCustom tokenizer + GPT2 demo finished.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(\"\\n== Algorithm 7: Preprocessing & Normalization Pipeline demo ==\")\ndef preprocess_pipeline(x_numpy):\n    # x_numpy: (T,12) or (12,T) - expect (12,T)\n    if x_numpy.ndim==2 and x_numpy.shape[0]==12:\n        X = x_numpy\n    else:\n        X = x_numpy.T\n    # baseline wander removal (high-pass via difference)\n    X = X - np.mean(X, axis=1, keepdims=True)\n    # robust scaling per lead (clip extreme)\n    med = np.median(X, axis=1, keepdims=True)\n    mad = np.median(np.abs(X - med), axis=1, keepdims=True) + 1e-6\n    Xs = (X - med) / mad\n    # bandpass (simple butterworth would be better); here do a simple rolling mean subtraction\n    window = 5\n    smooth = np.convolve(np.ones(window)/window, Xs[0], mode='same')\n    # return scaled\n    return Xs\n\n# quick check on one sample\nx0,_ = train_ds[0]\nprint(\"Preprocess sample shape:\", x0.shape)\nprint(\"Preprocess example (lead0 first 5) ->\", preprocess_pipeline(x0.numpy())[0,:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:30:23.416788Z","iopub.execute_input":"2025-09-23T14:30:23.417426Z","iopub.status.idle":"2025-09-23T14:30:23.443466Z","shell.execute_reply.started":"2025-09-23T14:30:23.417405Z","shell.execute_reply":"2025-09-23T14:30:23.442647Z"}},"outputs":[{"name":"stdout","text":"\n== Algorithm 7: Preprocessing & Normalization Pipeline demo ==\nPreprocess sample shape: torch.Size([12, 500])\nPreprocess example (lead0 first 5) -> [5.871397  4.8999734 5.014259  3.69998   2.7571282]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"\\n== Final: Train a simple ResNet1D classifier and print metrics ==\")\nclass SimpleResNet1D(nn.Module):\n    def __init__(self, out_dim=128, num_classes=len(CLASSES)):\n        super().__init__()\n        self.conv1 = nn.Conv1d(12,64,7,stride=2,padding=3)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n        self.pool = nn.AdaptiveAvgPool1d(128)\n        self.fc = nn.Linear(64*128, out_dim)\n        self.head = nn.Linear(out_dim, num_classes)\n    def forward(self,x):\n        h = self.relu(self.bn1(self.conv1(x)))\n        h = self.pool(h).flatten(1)\n        h = self.fc(h)\n        return self.head(h)\n\nresnet_clf = SimpleResNet1D().to(DEVICE)\nopt = torch.optim.Adam(resnet_clf.parameters(), lr=1e-4)\ncriterion = nn.BCEWithLogitsLoss()\n\nfor epoch in range(EPOCHS_CLASSIFIER):\n    resnet_clf.train(); running=0; n=0\n    for xb,yb in train_loader:\n        xb = xb.to(DEVICE)\n        yb = yb.to(DEVICE)\n        logits = resnet_clf(xb)\n        loss = criterion(logits, yb)\n        opt.zero_grad(); loss.backward(); opt.step()\n        running += loss.item(); n+=1\n    print(f\"ResNet clf epoch {epoch+1}, loss {running/max(1,n):.4f}\")\n\nprint(\"\\nEvaluate ResNet classifier on test set:\")\nYs, Ps, preds = None, None, None\nYs, Ps, preds = eval_multilabel(resnet_clf, test_loader)\n\n# detailed per-class report\ntry:\n    print(\"\\nClassification report (per-label) - threshold 0.5\")\n    y_true = Ys\n    y_pred = (Ps>0.5).astype(int)\n    for i,cls in enumerate(CLASSES):\n        print(f\"--- {cls} ---\")\n        print(classification_report(y_true[:,i], y_pred[:,i], zero_division=0))\nexcept Exception as e:\n    print(\"Error printing classification report:\", e)\n\nprint(\"\\nAll algorithms demo finished. Increase EPOCHS_* for better performance and replace fake captioning targets with real tokenized text for captioning/GPT training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T14:30:23.444329Z","iopub.execute_input":"2025-09-23T14:30:23.444572Z","iopub.status.idle":"2025-09-23T14:32:05.038083Z","shell.execute_reply.started":"2025-09-23T14:30:23.444545Z","shell.execute_reply":"2025-09-23T14:32:05.037095Z"}},"outputs":[{"name":"stdout","text":"\n== Final: Train a simple ResNet1D classifier and print metrics ==\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"ResNet clf epoch 1, loss 0.4723\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"ResNet clf epoch 2, loss 0.4206\n\nEvaluate ResNet classifier on test set:\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"AUROC(macro): 0.8011825252731727\nF1(macro): 0.42816122628209596\nExact-match accuracy: 0.43131868131868134\n\nClassification report (per-label) - threshold 0.5\n--- CD ---\n              precision    recall  f1-score   support\n\n         0.0       0.82      0.96      0.89      1674\n         1.0       0.71      0.31      0.43       510\n\n    accuracy                           0.81      2184\n   macro avg       0.77      0.63      0.66      2184\nweighted avg       0.79      0.81      0.78      2184\n\n--- HYP ---\n              precision    recall  f1-score   support\n\n         0.0       0.89      1.00      0.94      1937\n         1.0       0.40      0.02      0.03       247\n\n    accuracy                           0.89      2184\n   macro avg       0.64      0.51      0.49      2184\nweighted avg       0.83      0.89      0.84      2184\n\n--- MI ---\n              precision    recall  f1-score   support\n\n         0.0       0.79      0.96      0.87      1622\n         1.0       0.72      0.28      0.41       562\n\n    accuracy                           0.79      2184\n   macro avg       0.76      0.62      0.64      2184\nweighted avg       0.77      0.79      0.75      2184\n\n--- NORM ---\n              precision    recall  f1-score   support\n\n         0.0       0.87      0.74      0.80      1222\n         1.0       0.72      0.86      0.78       962\n\n    accuracy                           0.79      2184\n   macro avg       0.79      0.80      0.79      2184\nweighted avg       0.80      0.79      0.79      2184\n\n--- STTC ---\n              precision    recall  f1-score   support\n\n         0.0       0.84      0.89      0.87      1679\n         1.0       0.56      0.45      0.49       505\n\n    accuracy                           0.79      2184\n   macro avg       0.70      0.67      0.68      2184\nweighted avg       0.78      0.79      0.78      2184\n\n\nAll algorithms demo finished. Increase EPOCHS_* for better performance and replace fake captioning targets with real tokenized text for captioning/GPT training.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}