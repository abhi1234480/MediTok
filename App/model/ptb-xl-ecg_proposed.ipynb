{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ast, math, warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader as GeoDataLoader, Batch\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "\n",
    "# AMP compatibility (works with older and newer PyTorch)\n",
    "try:\n",
    "    from torch.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "except Exception:\n",
    "    from torch.cuda.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/\"\n",
    "METADATA_CSV = os.path.join(BASE_PATH, \"ptbxl_database.csv\")\n",
    "SCP_FILE = os.path.join(BASE_PATH, \"scp_statements.csv\")\n",
    "TARGET_SR = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 2\n",
    "PATIENCE = 7  # early stopping patience\n",
    "\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp_df = pd.read_csv(SCP_FILE, index_col=0)\n",
    "diagnostic_scp = scp_df[scp_df.diagnostic == 1]\n",
    "classes = sorted(diagnostic_scp.diagnostic_class.unique().tolist())\n",
    "NUM_CLASSES = len(classes)\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "def aggregate_superclass_from_scp_field(scp_codes_field):\n",
    "    try:\n",
    "        d = ast.literal_eval(scp_codes_field) if isinstance(scp_codes_field, str) else scp_codes_field\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for k in (d.keys() if isinstance(d, dict) else []):\n",
    "        if k in diagnostic_scp.index:\n",
    "            out.append(diagnostic_scp.loc[k].diagnostic_class)\n",
    "    return list(set(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anatomical_adjacency():\n",
    "    rel = [\n",
    "        (0,1),(1,2),\n",
    "        (0,4),(1,4),(2,5),(0,3),(3,4),(4,5),\n",
    "        (6,7),(7,8),(8,9),(9,10),(10,11),\n",
    "        (1,6),(5,10),(4,11)\n",
    "    ]\n",
    "    edges = []\n",
    "    for a,b in rel:\n",
    "        edges.append([a,b]); edges.append([b,a])\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "EDGE_INDEX = create_anatomical_adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists_for_row(base_path, fname):\n",
    "    # fname may already contain 'records100/00000/00001_lr.dat' or without extension\n",
    "    full = os.path.join(base_path, fname)\n",
    "    base, ext = os.path.splitext(full)\n",
    "    # Check base.dat + base.hea\n",
    "    if os.path.exists(base + \".dat\") and os.path.exists(base + \".hea\"):\n",
    "        return True, base\n",
    "    # maybe 'fname' already had .dat extension included\n",
    "    if os.path.exists(full) and (full.endswith(\".dat\") or full.endswith(\".hea\")):\n",
    "        # normalize to base without extension\n",
    "        base2 = os.path.splitext(full)[0]\n",
    "        if os.path.exists(base2 + \".dat\") and os.path.exists(base2 + \".hea\"):\n",
    "            return True, base2\n",
    "    # fallback: return False and proposed base\n",
    "    return False, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBXLGraphDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, base_path: str, use_lr: bool = True, augment: bool=False):\n",
    "        self.rows = []\n",
    "        missing = 0\n",
    "        for _, row in df.iterrows():\n",
    "            fname = row['filename_lr'] if use_lr else row['filename_hr']\n",
    "            ok, basefile = file_exists_for_row(base_path, fname)\n",
    "            if ok:\n",
    "                self.rows.append((row, basefile))\n",
    "            else:\n",
    "                missing += 1\n",
    "        if missing>0:\n",
    "            print(f\"Warning: {missing} records missing files (skipped).\")\n",
    "        self.edge_index = EDGE_INDEX\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self): return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, basefile = self.rows[idx]\n",
    "        # wfdb accepts basefile without extension\n",
    "        rec, _ = wfdb.rdsamp(basefile)\n",
    "        sig = rec.astype(np.float32)  # (T, 12)\n",
    "        # some records may be transposed; ensure shape (T,12)\n",
    "        if sig.ndim == 1:\n",
    "            sig = sig[:, None]\n",
    "        if sig.shape[1] != 12 and sig.shape[0] == 12:\n",
    "            sig = sig.T\n",
    "        if sig.shape[1] != 12:\n",
    "            # pad/truncate to 12 leads\n",
    "            if sig.shape[1] < 12:\n",
    "                sig = np.pad(sig, ((0,0),(0, 12 - sig.shape[1])), mode='constant')\n",
    "            else:\n",
    "                sig = sig[:, :12]\n",
    "\n",
    "        # normalize per lead\n",
    "        sig = (sig - sig.mean(axis=0)) / (sig.std(axis=0) + 1e-8)\n",
    "        processed = sig.T  # (12, T)\n",
    "\n",
    "        # augment (mild)\n",
    "        if self.augment:\n",
    "            if np.random.rand() < 0.25:\n",
    "                processed = processed + 0.01 * np.random.randn(*processed.shape)\n",
    "            if np.random.rand() < 0.25:\n",
    "                shift = int(np.random.randint(-int(0.05 * processed.shape[1]), int(0.05 * processed.shape[1]) + 1))\n",
    "                processed = np.roll(processed, shift, axis=1)\n",
    "\n",
    "        # target: try 'diagnostic_superclass' field first (if stored), else build from scp_codes\n",
    "        labels_list = []\n",
    "        if 'diagnostic_superclass' in row.index and isinstance(row['diagnostic_superclass'], (list, tuple, np.ndarray)):\n",
    "            labels_list = list(row['diagnostic_superclass'])\n",
    "        else:\n",
    "            try:\n",
    "                labels_list = aggregate_superclass_from_scp_field(row['scp_codes'])\n",
    "            except Exception:\n",
    "                labels_list = []\n",
    "\n",
    "        y = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "        for c in labels_list:\n",
    "            if c in classes:\n",
    "                y[classes.index(c)] = 1.0\n",
    "\n",
    "        # Data.x: shape (num_nodes=12, seq_len)\n",
    "        data = Data(x=torch.tensor(processed, dtype=torch.float), edge_index=self.edge_index, y=torch.tensor(y, dtype=torch.float))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel=7):\n",
    "        super().__init__()\n",
    "        pad = (kernel-1)//2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel, padding=pad),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_ch, out_ch, kernel, padding=pad),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "        )\n",
    "        self.res = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x) + self.res(x))\n",
    "\n",
    "class PerNodeCNN(nn.Module):\n",
    "    def __init__(self, out_dim=48):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            Residual1D(1, 16),\n",
    "            Residual1D(16, 32),\n",
    "            Residual1D(32, 48)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(48, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (total_nodes, seq_len)\n",
    "        x = x.unsqueeze(1)          # (total_nodes, 1, seq_len)\n",
    "        x = self.net(x)            # (total_nodes, channels, seq_len)\n",
    "        x = self.pool(x).squeeze(-1)  # (total_nodes, channels)\n",
    "        return self.fc(x)          # (total_nodes, out_dim)\n",
    "\n",
    "class GNNHybrid(nn.Module):\n",
    "    def __init__(self, num_classes, node_feat_dim=48, gnn_hidden=64):\n",
    "        super().__init__()\n",
    "        self.node_cnn = PerNodeCNN(node_feat_dim)\n",
    "        self.gnn1 = SAGEConv(node_feat_dim, gnn_hidden)\n",
    "        self.gnn2 = SAGEConv(gnn_hidden, gnn_hidden)\n",
    "        self.classifier = nn.Sequential(nn.Linear(gnn_hidden, 128), nn.ReLU(), nn.Dropout(0.25), nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data.x: (total_nodes, seq_len)\n",
    "        x = self.node_cnn(data.x.to(DEVICE))   # (total_nodes, node_feat_dim)\n",
    "        edge_index = data.edge_index.to(DEVICE)\n",
    "        batch = data.batch.to(DEVICE) if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=DEVICE)\n",
    "        x = F.relu(self.gnn1(x, edge_index))\n",
    "        x = F.relu(self.gnn2(x, edge_index))\n",
    "        g = global_mean_pool(x, batch)   # (batch_size, gnn_hidden)\n",
    "        out = self.classifier(g)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma = alpha, gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    def forward(self, logits, targets):\n",
    "        bce = self.bce(logits, targets)\n",
    "        pt = torch.exp(-bce)\n",
    "        loss = self.alpha * (1 - pt)**self.gamma * bce\n",
    "        return loss.mean()\n",
    "\n",
    "def reshape_batch_y_for_loss(batch_y, num_graphs):\n",
    "    \"\"\"Ensure batch_y becomes (num_graphs, num_classes). Robust to PyG batching quirks.\"\"\"\n",
    "    if not isinstance(batch_y, torch.Tensor):\n",
    "        batch_y = torch.tensor(batch_y)\n",
    "    # If already shaped (num_graphs, num_classes) -> good\n",
    "    if batch_y.dim() == 2 and batch_y.shape[0] == num_graphs:\n",
    "        return batch_y\n",
    "    # If 1D flattened of length num_graphs * num_classes\n",
    "    if batch_y.dim() == 1 and batch_y.numel() == num_graphs * NUM_CLASSES:\n",
    "        return batch_y.view(num_graphs, NUM_CLASSES)\n",
    "    # If 2D but shape (num_graphs * something, ) -> reshape\n",
    "    try:\n",
    "        return batch_y.reshape(num_graphs, -1)\n",
    "    except Exception:\n",
    "        # As last resort, repeat/trim\n",
    "        y = batch_y.flatten()\n",
    "        y = y[:num_graphs * NUM_CLASSES]\n",
    "        return y.view(num_graphs, NUM_CLASSES)\n",
    "\n",
    "def evaluate_model(model, loader, thresholds=None):\n",
    "    model.eval()\n",
    "    Ys, Ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            # model expects a Batch\n",
    "            try:\n",
    "                ctx = _autocast(device_type=DEVICE.type, enabled=(DEVICE.type == \"cuda\"))\n",
    "            except TypeError:\n",
    "                ctx = _autocast(enabled=(DEVICE.type == \"cuda\"))\n",
    "            with ctx:\n",
    "                logits = model(batch)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            # obtain y shape robustly\n",
    "            y_np = batch.y.cpu().numpy()\n",
    "            if y_np.ndim == 1 and y_np.size == batch.num_graphs * NUM_CLASSES:\n",
    "                y_np = y_np.reshape(batch.num_graphs, NUM_CLASSES)\n",
    "            elif y_np.ndim == 2 and y_np.shape[0] != batch.num_graphs:\n",
    "                y_np = y_np.reshape(batch.num_graphs, -1)\n",
    "            Ys.append(y_np)\n",
    "            Ps.append(probs)\n",
    "    if len(Ys) == 0:\n",
    "        return float('nan'), float('nan'), None, None\n",
    "    Y = np.vstack(Ys); P = np.vstack(Ps)\n",
    "    # AUROC robust\n",
    "    try:\n",
    "        auc = roc_auc_score(Y, P, average='macro')\n",
    "    except Exception:\n",
    "        aucs = []\n",
    "        for i in range(Y.shape[1]):\n",
    "            try:\n",
    "                aucs.append(roc_auc_score(Y[:,i], P[:,i]))\n",
    "            except Exception:\n",
    "                pass\n",
    "        auc = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
    "    if thresholds is None:\n",
    "        preds = (P > 0.5).astype(int)\n",
    "    else:\n",
    "        preds = (P > np.array(thresholds)).astype(int)\n",
    "    f1 = f1_score(Y, preds, average='macro', zero_division=0)\n",
    "    return auc, f1, Y, P\n",
    "\n",
    "def tune_thresholds(y_true, y_pred):\n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        best_t, best_f1 = 0.5, -1.0\n",
    "        for t in np.linspace(0.1, 0.9, 17):\n",
    "            f = f1_score(y_true[:,i], (y_pred[:,i] > t).astype(int), zero_division=0)\n",
    "            if f > best_f1:\n",
    "                best_f1, best_t = f, t\n",
    "        thresholds.append(best_t)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(metadata_csv=METADATA_CSV, base_path=BASE_PATH):\n",
    "    df = pd.read_csv(metadata_csv, index_col='ecg_id')\n",
    "    # ensure diagnostic_superclass exists\n",
    "    if 'diagnostic_superclass' not in df.columns:\n",
    "        df['diagnostic_superclass'] = df['scp_codes'].apply(aggregate_superclass_from_scp_field)\n",
    "    # prefer official folds if available\n",
    "    if 'strat_fold' in df.columns:\n",
    "        train_df = df[df.strat_fold.isin(range(1,9))].copy()\n",
    "        val_df   = df[df.strat_fold == 9].copy()\n",
    "        test_df  = df[df.strat_fold == 10].copy()\n",
    "    else:\n",
    "        # fallback random splits\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_val, test_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "        train_df, val_df = train_test_split(train_val, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "    train_ds = PTBXLGraphDataset(train_df, base_path, use_lr=True, augment=True)\n",
    "    val_ds   = PTBXLGraphDataset(val_df, base_path, use_lr=True, augment=False)\n",
    "    test_ds  = PTBXLGraphDataset(test_df, base_path, use_lr=True, augment=False)\n",
    "\n",
    "    train_loader = GeoDataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = GeoDataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = GeoDataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save():\n",
    "    train_loader, val_loader, test_loader = build_loaders()\n",
    "    model = GNNHybrid(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    print(model)\n",
    "    criterion = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    # GradScaler robust init\n",
    "    try:\n",
    "        scaler = _GradScaler('cuda' if DEVICE.type == 'cuda' else 'cpu')\n",
    "    except TypeError:\n",
    "        scaler = _GradScaler(enabled=(DEVICE.type=='cuda'))\n",
    "\n",
    "    best_auc = -1.0\n",
    "    patience_cnt = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_graphs = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(DEVICE)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            # robust autocast usage\n",
    "            try:\n",
    "                ctx = _autocast(device_type=DEVICE.type, enabled=(DEVICE.type==\"cuda\"))\n",
    "            except TypeError:\n",
    "                ctx = _autocast(enabled=(DEVICE.type==\"cuda\"))\n",
    "            with ctx:\n",
    "                logits = model(batch)  # (batch_size, num_classes)\n",
    "                # ensure batch.y shaped (batch_size, num_classes)\n",
    "                target = batch.y\n",
    "                target = reshape_batch_y_for_loss(target, batch.num_graphs).to(DEVICE)\n",
    "                loss = criterion(logits, target)\n",
    "            scaler.scale(loss).backward()\n",
    "            # gradient clipping (helpful)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item() * batch.num_graphs\n",
    "            n_graphs += batch.num_graphs\n",
    "            pbar.set_postfix({'loss': f\"{running_loss/max(1,n_graphs):.4f}\"})\n",
    "\n",
    "        # validation\n",
    "        val_auc, val_f1, val_Y, val_P = evaluate_model(model, val_loader)\n",
    "        scheduler.step(val_auc if not math.isnan(val_auc) else 0.0)\n",
    "        print(f\"Epoch {epoch}  train_loss={running_loss/max(1,n_graphs):.4f}  val_auc={val_auc:.4f}  val_f1={val_f1:.4f}\")\n",
    "\n",
    "        if not math.isnan(val_auc) and val_auc > best_auc + 1e-5:\n",
    "            best_auc = val_auc\n",
    "            patience_cnt = 0\n",
    "            torch.save(model.state_dict(), \"best_model_gnn.pth\")\n",
    "            print(\"  -> saved best_model_gnn.pth\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # load best and tune thresholds\n",
    "    print(\"Loading best model for final evaluation and threshold tuning...\")\n",
    "    model.load_state_dict(torch.load(\"best_model_gnn.pth\", map_location=DEVICE))\n",
    "    _, _, Yv, Pv = evaluate_model(model, val_loader)\n",
    "    thresholds = tune_thresholds(Yv, Pv)\n",
    "    test_auc, test_f1, Yt, Pt = evaluate_model(model, test_loader, thresholds=thresholds)\n",
    "    print(f\"[TEST] AUROC={test_auc:.4f}  MacroF1={test_f1:.4f}\")\n",
    "    print(classification_report(Yt, (Pt > np.array(thresholds)).astype(int), target_names=classes, zero_division=0))\n",
    "    torch.save(model.state_dict(), \"ptbxl_gnn_hybrid_final.pth\")\n",
    "    print(\"Final model saved as ptbxl_gnn_hybrid_final.pth\")\n",
    "    return model, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T08:48:42.746216Z",
     "iopub.status.busy": "2025-09-22T08:48:42.745410Z",
     "iopub.status.idle": "2025-09-22T09:11:26.255833Z",
     "shell.execute_reply": "2025-09-22T09:11:26.255035Z",
     "shell.execute_reply.started": "2025-09-22T08:48:42.746182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Classes: ['CD', 'HYP', 'MI', 'NORM', 'STTC']\n",
      "GNNHybrid(\n",
      "  (node_cnn): PerNodeCNN(\n",
      "    (net): Sequential(\n",
      "      (0): Residual1D(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv1d(1, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (res): Conv1d(1, 16, kernel_size=(1,), stride=(1,))\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (1): Residual1D(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (res): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (2): Residual1D(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv1d(32, 48, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "          (3): Conv1d(48, 48, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "          (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (res): Conv1d(32, 48, kernel_size=(1,), stride=(1,))\n",
      "        (act): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (pool): AdaptiveAvgPool1d(output_size=1)\n",
      "    (fc): Linear(in_features=48, out_features=48, bias=True)\n",
      "  )\n",
      "  (gnn1): SAGEConv(48, 64, aggr=mean)\n",
      "  (gnn2): SAGEConv(64, 64, aggr=mean)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  train_loss=0.1113  val_auc=0.8368  val_f1=0.4589\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  train_loss=0.0950  val_auc=0.8526  val_f1=0.4924\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  train_loss=0.0891  val_auc=0.8705  val_f1=0.6429\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4  train_loss=0.0842  val_auc=0.8885  val_f1=0.6144\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5  train_loss=0.0807  val_auc=0.8870  val_f1=0.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6  train_loss=0.0804  val_auc=0.8871  val_f1=0.5925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7  train_loss=0.0792  val_auc=0.8929  val_f1=0.6164\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  train_loss=0.0772  val_auc=0.9002  val_f1=0.6525\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9  train_loss=0.0756  val_auc=0.9018  val_f1=0.6757\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  train_loss=0.0755  val_auc=0.9007  val_f1=0.6335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11  train_loss=0.0755  val_auc=0.9005  val_f1=0.6638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12  train_loss=0.0732  val_auc=0.9055  val_f1=0.6445\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13  train_loss=0.0729  val_auc=0.9050  val_f1=0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14  train_loss=0.0731  val_auc=0.9028  val_f1=0.6622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15  train_loss=0.0728  val_auc=0.9086  val_f1=0.6821\n",
      "  -> saved best_model_gnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16  train_loss=0.0721  val_auc=0.9065  val_f1=0.6584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17  train_loss=0.0728  val_auc=0.9033  val_f1=0.6128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18  train_loss=0.0723  val_auc=0.9041  val_f1=0.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19  train_loss=0.0721  val_auc=0.9084  val_f1=0.6822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20  train_loss=0.0692  val_auc=0.9123  val_f1=0.6865\n",
      "  -> saved best_model_gnn.pth\n",
      "Loading best model for final evaluation and threshold tuning...\n",
      "[TEST] AUROC=0.9069  MacroF1=0.7057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.83      0.62      0.71       498\n",
      "         HYP       0.35      0.68      0.46       263\n",
      "          MI       0.73      0.78      0.75       553\n",
      "        NORM       0.77      0.93      0.85       964\n",
      "        STTC       0.68      0.86      0.76       523\n",
      "\n",
      "   micro avg       0.69      0.81      0.74      2801\n",
      "   macro avg       0.67      0.77      0.71      2801\n",
      "weighted avg       0.72      0.81      0.75      2801\n",
      " samples avg       0.73      0.81      0.75      2801\n",
      "\n",
      "Final model saved as ptbxl_gnn_hybrid_final.pth\n"
     ]
    }
   ],
   "source": [
    "def predict_dathea(basefile_noext, model, thresholds=None):\n",
    "    base = os.path.splitext(basefile_noext)[0]\n",
    "    rec, _ = wfdb.rdsamp(base)\n",
    "    sig = rec.astype(np.float32)\n",
    "    if sig.ndim == 1: sig = sig[:, None]\n",
    "    if sig.shape[1] != 12 and sig.shape[0] == 12: sig = sig.T\n",
    "    if sig.shape[1] != 12:\n",
    "        sig = np.pad(sig, ((0,0),(0, max(0, 12 - sig.shape[1]))))\n",
    "        sig = sig[:, :12]\n",
    "    sig = (sig - sig.mean(axis=0)) / (sig.std(axis=0) + 1e-8)\n",
    "    processed = sig.T\n",
    "    data = Data(x=torch.tensor(processed, dtype=torch.float), edge_index=EDGE_INDEX)\n",
    "    batch = Batch.from_data_list([data]).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            ctx = _autocast(device_type=DEVICE.type, enabled=(DEVICE.type==\"cuda\"))\n",
    "        except TypeError:\n",
    "            ctx = _autocast(enabled=(DEVICE.type==\"cuda\"))\n",
    "        with ctx:\n",
    "            logits = model(batch)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    if thresholds is None:\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "    else:\n",
    "        preds = (probs > np.array(thresholds)).astype(int)\n",
    "    result = {classes[i]: float(probs[i]) for i in range(NUM_CLASSES)}\n",
    "    predicted = [classes[i] for i in range(NUM_CLASSES) if preds[i] == 1]\n",
    "    return result, predicted\n",
    "\n",
    "model, thresholds = train_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:12:38.654269Z",
     "iopub.status.busy": "2025-09-22T09:12:38.653912Z",
     "iopub.status.idle": "2025-09-22T09:12:38.822752Z",
     "shell.execute_reply": "2025-09-22T09:12:38.822004Z",
     "shell.execute_reply.started": "2025-09-22T09:12:38.654237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:13:41.022547Z",
     "iopub.status.busy": "2025-09-22T09:13:41.021715Z",
     "iopub.status.idle": "2025-09-22T09:13:42.925189Z",
     "shell.execute_reply": "2025-09-22T09:13:42.924180Z",
     "shell.execute_reply.started": "2025-09-22T09:13:41.022512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip freeze>req.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1136210,
     "sourceId": 1905968,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
